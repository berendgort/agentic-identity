# Personal Branding & Positioning

How to position yourself as a Research Engineer for DeepMind, Meta, NVIDIA.

---

## üéØ Core Positioning

**Role:** Research Engineer (Research + Production Systems Hybrid)  
**Domain:** LLM Systems, Agent Evaluation, Production ML Infrastructure  
**Unique Angle:** Build agent systems that empower humans + ship at scale

---

## üìù LinkedIn Profile (Universal Version)

### Headline
```
Research Engineer | LLM Systems & Agent Evaluation | PhD Candidate
```

### Summary
```
I bridge research and production systems‚Äîbuilding LLM infrastructure that scales while advancing agent capabilities through novel evaluation frameworks.

**Research:** Published 5+ papers on transformers, attention mechanisms, and agent systems. Built custom LLM evaluation frameworks and developed AgentEdge multi-agent framework (2.76√ó improvement in orchestration success).

**Engineering:** Deployed production inference serving 425 req/sec with 99.99% uptime using NVIDIA Triton. Optimized models reducing latency 67% and energy consumption 13%. Secured MareNostrum 5 HPC allocation for large-scale training.

**Currently:** PhD in AI at Universitat Polit√®cnica de Catalunya (finishing March 2026), focusing on LLM-powered agent systems and production ML infrastructure.

üõ† Core Skills: PyTorch, LLM Evaluation, Multi-Agent Systems, Production Inference (Triton), MLOps, Kubernetes, HPC, Transformer Architectures

üìö 5+ publications | Active open-source contributor | Passionate about systems that empower humans
```

### Experience Bullets (Reframed)

**Current Role - Nearby Computing:**
- Built custom LLM evaluation frameworks for agent capabilities in production environments
- Deployed production inference pipeline serving 425 req/sec with 99.99% uptime using NVIDIA Triton
- Developed AgentEdge multi-agent framework improving orchestration success rates by 2.76√ó
- Created AERO ultra-lightweight model (sub-1K parameters) reducing inference latency by 67%
- Secured MareNostrum 5 HPC allocation for large-scale LLM training and optimization
- Engineered production MLOps platform with MLflow and PostgreSQL achieving 99.99% uptime

---

## üé§ Interview Pitches (Company-Specific)

### For Anthropic (Research Engineer, Tool Use) ‚≠ê TOP FIT
```
I'm a Research Engineer specializing in agent systems and LLM evaluation. I built AgentEdge‚Äîa multi-agent framework that improved orchestration success rates by 2.76√ó‚Äîand developed custom evaluation frameworks to assess LLM capabilities in tool use, planning, and multi-step reasoning.

What excites me about Anthropic's Tool Use team is the focus on agent safety and controllability. I've been thinking deeply about how to evaluate and ensure reliable agent behavior‚Äîexactly what you need as Claude scales to handle increasingly complex workflows. My production experience (425 req/sec, 99.99% uptime) means I understand the practical constraints of shipping agent systems at scale.

I'm particularly interested in developing rigorous evaluation frameworks for agent capabilities and contributing to the safety research that makes these systems trustworthy.
```

### For DeepMind (Research Scientist, LLMs)
```
I'm a Research Engineer working at the intersection of LLM capabilities and agent systems. I've published 5+ papers and built the AgentEdge framework‚Äîa multi-agent system that improved orchestration success rates by 2.76√ó. 

What makes my work unique is the focus on *controllability*‚ÄîI developed custom evaluation frameworks to measure how well LLMs perform as reasoning agents, assessing planning, tool-use, and multi-step decision-making. I've also deployed these systems at scale, serving 425 requests/second in production.

I'm excited about DeepMind's mission to empower humans rather than replace them. My PhD research on agent systems aligns perfectly with your team's focus on controllability and practical applications.
```

### For Meta (Research Scientist, Systems/ML)
```
I'm a Research Engineer who ships production systems and publishes research. Over the past 2+ years, I've built production ML infrastructure serving 425 inferences/second with 99.99% uptime using NVIDIA Triton, Kubernetes, and MLOps best practices.

On the research side, I've published 5+ papers on transformer architectures and agent systems. I developed AgentEdge‚Äîa multi-agent framework that improved success rates 2.76√ó‚Äîand created novel lightweight models (AERO, OmniFORE) that reduced inference latency by 67%.

I'm excited about Meta because you value the hybrid of strong engineering and research impact. I contribute to open source, I build systems that scale, and I publish‚Äîexactly what you're looking for. I want to work on problems where I can push both the research frontier and ship systems that billions of people use.
```

### For NVIDIA (Deep Learning Engineer, LLM Evaluation)
```
I've spent the past 2+ years building production LLM infrastructure at scale. I architected an inference pipeline handling 425 requests/second using NVIDIA Triton, and I've optimized models to reduce latency by 67% while cutting energy consumption by 13%.

My PhD research involved building custom LLM evaluation frameworks from scratch‚Äîdesigning metrics, validating model performance, and assessing flagship models on complex reasoning tasks. I've also secured access to MareNostrum 5 HPC (hundreds of petaFLOPS) for large-scale training and optimization.

What excites me about NVIDIA is the opportunity to work on the frontier of LLM evaluation at petaFLOP scale, developing methodologies to assess the next generation of foundation models. I want to help bring Gemma, Llama-3, and future models to production as optimized NIMs.
```

### For Groq (Senior ML Engineer, Post Training)
```
I'm a Research Engineer with production ML experience optimizing models for fast inference. I've reduced model latency by 67% through compression techniques and deployed systems serving 425 requests/second with 99.99% uptime.

My work includes training and optimizing flagship models (Llama, Gemma, Mistral) and building production inference pipelines. I have hands-on experience with quantization, model compression, and performance optimization‚Äîexactly what Groq needs for fast LPU-based inference.

I'm excited about pushing the boundaries of inference speed and making high-performance AI compute more accessible.
```

---

## üìÑ Resume Bullets (Reframing Guide)

### ‚ùå Before (Network-Focused)
- Built inference pipeline scaling to 425 inferences per second using NVIDIA Triton
- Developed AgentEdge multi-agent framework
- Created AERO lightweight forecasting model
- PhD in AI for 6G Networking

### ‚úÖ After (LLM-Focused)
- Architected high-throughput LLM inference pipeline achieving 425 req/sec with NVIDIA Triton, optimizing latency and cost for production workloads
- Engineered AgentEdge framework for LLM-powered autonomous agents, improving reasoning success rates by 2.76√ó
- Designed AERO ultra-lightweight neural architecture (sub-1K parameters) reducing inference latency by 67%‚Äîdemonstrating extreme model compression techniques
- PhD in AI with focus on transformer architectures and inference optimization for resource-constrained environments

---

## üéØ Company-Specific Positioning

### Anthropic ‚≠ê BEST FIT
**Angle:** "Build safe agent systems with rigorous evaluation"

**Emphasize:**
- AgentEdge tool use framework
- Custom evaluation for agent capabilities
- Agent safety & controllability
- Research + production hybrid

**Resume Focus:**
- Agent systems (AgentEdge 2.76√ó improvement)
- Evaluation frameworks for tool use
- Production deployment at scale
- Safety-oriented research

### NVIDIA
**Angle:** "Optimize LLM inference + build evaluation frameworks"

**Emphasize:**
- Triton inference experience (425 req/sec)
- Custom evaluation frameworks
- HPC experience (MareNostrum 5)
- Model optimization expertise

**Resume Focus:**
- NVIDIA Triton expertise
- Evaluation methodologies
- HPC clusters
- Performance optimization (67% latency reduction)

### Meta
**Angle:** "Ship production ML systems at scale + publish research"

**Emphasize:**
- Production systems (425 req/sec, 99.99% uptime)
- Open source contributions (BUILD THIS)
- Software engineering + research
- Systems at scale

**Resume Focus:**
- Production infrastructure
- Cross-functional collaboration
- PyTorch/systems programming
- Scalability achievements

### DeepMind
**Angle:** "Build agent systems that empower humans"

**Emphasize:**
- Agent systems research
- Controllability & interpretability
- Custom evaluation frameworks
- Team-oriented research

**Resume Focus:**
- Publications on agent systems
- Evaluation methodologies
- System building/prototyping
- Research depth

### Groq
**Angle:** "Optimize models for ultra-fast inference"

**Emphasize:**
- Model compression (67% latency reduction)
- Production optimization
- Quantization expertise
- Inference speed focus

**Resume Focus:**
- Model optimization achievements
- Production inference (425 req/sec)
- Quantization/compression techniques
- Engineering excellence

---

## üö´ What to De-emphasize

**Avoid mentioning:**
- "6G Networking"
- "Network Orchestration" (unless asked)
- "Edge-Cloud Computing" (frame as "distributed inference")
- "Telecommunications"

**Instead, say:**
- "LLM Systems"
- "Production ML Infrastructure"
- "Distributed Inference"
- "Agent Evaluation Frameworks"

---

## üíº Tailored Resume Tips

### All Companies
- Lead with Research + Engineering hybrid
- Show both publications AND production systems
- Quantify impact (425 req/sec, 2.76√ó, 67% reduction)
- PyTorch prominent

### DeepMind-Specific
- Publications section at top
- Agent systems highlighted
- Team collaboration emphasized
- "Empowering humans" language

### Meta-Specific
- Production systems at top
- Open source contributions
- Software engineering skills
- Scalability metrics

### NVIDIA-Specific
- Triton experience prominently placed
- Evaluation frameworks highlighted
- HPC experience emphasized
- Performance optimization results

---

## üìß Cover Letter Template

```
Dear [Hiring Manager],

I'm writing to apply for the [Role] position at [Company]. As a Research Engineer finishing my PhD in AI, I bridge research and production‚Äîbuilding LLM systems that scale while advancing agent capabilities.

[Company-Specific Paragraph - See above pitches]

My background combines:
‚Ä¢ Research: 5+ publications on transformers and agent systems
‚Ä¢ Engineering: Production inference at 425 req/sec with 99.99% uptime
‚Ä¢ Innovation: Custom LLM evaluation frameworks and AgentEdge (2.76√ó improvement)
‚Ä¢ Scale: Experience with MareNostrum 5 HPC (petaFLOP scale)

I'm excited about [Company's specific mission/project] and would love to contribute to [specific team/initiative]. I've started building public projects in LLM evaluation [link to GitHub] and am actively contributing to [lm-eval/other repo].

Thank you for considering my application. I look forward to discussing how I can contribute to [Company's] mission.

Best regards,
Berend Gort
```

---

## üéØ Positioning Summary

**You are:** A Research Engineer (research + production hybrid)

**You are NOT:**
- A network engineer learning AI
- A pure researcher who can't code
- A pure engineer without publications

**Your unique value:**
- Custom LLM evaluation frameworks (domain-specific ‚Üí general)
- Agent systems that empower humans
- Production systems at scale (Triton, HPC, MLOps)
- Research publications + PhD

**Your story:** Repositioning niche expertise (network orchestration) for broader LLM impact. Same skills, different audience.

üöÄ **Make them visible!**

